# train_stage3_conditional_mdn.py

import argparse
import os
import json
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

# --- å¯¼å…¥æ‰€æœ‰å¿…è¦çš„æ¨¡å— ---
from models.fomo_joint_autoencoder import FomoJointAutoencoder
from FoMo.model_zoo.multimodal_mae import MultiSpectralViT
from models.disentangler import FeatureDisentangler
from models.conditional_mdn import ConditionalMDN, get_timestep_embedding  # å¯¼å…¥æ–°æ¨¡å‹
from dataset.fomo_dataset import FomoCompatibleDataset


# from diffusers.schedulers.scheduling_ddpm import DDPMScheduler # å¯ä»¥ä½¿ç”¨diffusersåº“æ¥ç®¡ç†æ‰©æ•£è¿‡ç¨‹

def train_stage3_conditional_mdn(args):
    device = torch.device(args.device)

    # --- 1. åŠ è½½å¹¶å†»ç»“ç¬¬ä¸€ã€äºŒé˜¶æ®µçš„æ¨¡å‹ ---
    print("Loading and freezing pretrained models from Stage 1 & 2...")
    # a. åŠ è½½è‡ªç¼–ç å™¨ (Stage 1)
    # ... (ä¸ç¬¬äºŒé˜¶æ®µåŠ è½½é€»è¾‘ç›¸åŒ) ...
    modality_keys = sorted([int(k) for k in config['modality_channels'].keys()])
    num_total_bands = max(modality_keys) + 1
    modality_channels_for_init = list(range(num_total_bands))
    fomo_init_configs = {'single_embedding_layer': True, 'modality_channels': modality_channels_for_init}

    fomo_encoder = MultiSpectralViT(
        image_size=args.image_size, patch_size=args.patch_size, channels=1, num_classes=1000,
        dim=args.encoder_dim, depth=args.encoder_depth, heads=args.encoder_heads, mlp_dim=args.encoder_mlp_dim,
        configs=fomo_init_configs
    )

    autoencoder = FomoJointAutoencoder(
        fomo_encoder=fomo_encoder, decoder_dim=args.decoder_dim, decoder_depth=args.decoder_depth,
        sar_channels=args.sar_channels, opt_channels=args.opt_channels,
        image_size=args.image_size, patch_size=args.patch_size
    )

    # b. åŠ è½½ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå¥½çš„æƒé‡
    print(f"Loading Stage 1 checkpoint from: {args.stage1_ckpt_path}")
    autoencoder.load_state_dict(torch.load(args.stage1_ckpt_path, map_location='cpu'))

    # b. åŠ è½½è§£è€¦å™¨ (Stage 2)
    disentangler = FeatureDisentangler(...)
    disentangler.load_state_dict(torch.load(args.stage2_ckpt_path))
    disentangler.eval().to(device)
    print("All pretrained models are loaded and frozen.")

    # --- 2. åˆå§‹åŒ–éœ€è¦è®­ç»ƒçš„æ–°æ¨¡å‹ï¼šC-MDN ---
    print("Initializing Stage 3 model (Conditional MDN)...")
    c_mdn = ConditionalMDN(
        feature_dim=args.encoder_dim,
        condition_dim=args.encoder_dim,  # å†…å®¹å’Œé£æ ¼ç‰¹å¾ç»´åº¦ç›¸åŒ
        # ... å…¶ä»–è¶…å‚æ•° ...
    ).to(device)

    # --- 3. æ•°æ®å‡†å¤‡ (åªéœ€è¦å…‰å­¦æ•°æ®) ---
    print("Preparing Optical dataset for training...")
    transform_sar = transforms.Compose([
        transforms.Resize((args.image_size, args.image_size)),
        transforms.Grayscale(num_output_channels=3),  # !! å°†ç°åº¦å›¾è½¬æ¢ä¸º3é€šé“ !!
        transforms.ToTensor(),
        # transforms.Normalize(mean=[0.5], std=[0.5])
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    transform_opt = transforms.Compose([
        transforms.Resize((args.image_size, args.image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    sar_dataset = FomoCompatibleDataset(
        data_dir=args.sar_data_dir, dataset_name='my_sar_train_dataset',
        config=config, transform=transform_sar, in_chans=args.sar_channels, modality_label=0  # SARæ ‡ç­¾ä¸º0
    )
    opt_dataset = FomoCompatibleDataset(
        data_dir=args.opt_data_dir, dataset_name='my_optical_train_dataset',
        config=config, transform=transform_opt, in_chans=args.opt_channels, modality_label=1  # OPTæ ‡ç­¾ä¸º1
    )

    # --- 4. è®¾ç½®æ‰©æ•£è¿‡ç¨‹ ---
    noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2")

    # --- 5. ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•° ---
    optimizer = torch.optim.AdamW(c_mdn.parameters(), lr=args.learning_rate)
    criterion_mse = nn.MSELoss()

    print("Starting Stage 3: Conditional MDN Training...")
    for epoch in range(args.num_epochs):
        c_mdn.train()

        for images, keys in tqdm(dataloader, ...):
            images = images.to(device)

        # --- a. æå–å†»ç»“çš„ç‰¹å¾ ---
        with torch.no_grad():
            mixed_tokens_opt = autoencoder.encoder((images, keys), pool=False)
            unrelated_opt, related_opt_0 = disentangler(mixed_tokens_opt)

        # --- b. å‡†å¤‡æ‰©æ•£æ¨¡å‹è¾“å…¥(ä¸¥æ ¼éµå¾ª Epsilon-Prediction) ---

        # 1. é‡‡æ ·å™ªå£° epsilon (Îµ)
        # epsilon çš„å½¢çŠ¶ä¸å¹²å‡€çš„é£æ ¼ç‰¹å¾å®Œå…¨ç›¸åŒ
        epsilon = torch.randn_like(related_opt_0)
        bsz = related_opt_0.shape[0]

        # 2. é‡‡æ ·æ—¶é—´æ­¥
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device).long()

        # 3. å¯¹å¹²å‡€çš„é£æ ¼ç‰¹å¾åŠ å™ªï¼Œå¾—åˆ° xâ‚œ
        # noise_scheduler.add_noise ä¼šè‡ªåŠ¨å¤„ç†alpha, betaç­‰å¤æ‚çš„ç³»æ•°
        noisy_related_opt_t = noise_scheduler.add_noise(related_opt_0, epsilon, timesteps)

        # --- c. è®­ç»ƒC-MDN ---
        optimizer.zero_grad()

        # 1. æ¨¡å‹é¢„æµ‹ epsilon_pred
        # æ¨¡å‹çš„è¾“å…¥æ˜¯å¸¦å™ªå£°çš„æ•°æ® xâ‚œï¼Œæ—¶é—´æ­¥ tï¼Œä»¥åŠæ¡ä»¶ c
        epsilon_pred = c_mdn(
            noisy_style_tokens=noisy_related_opt_t,
            time_steps=timesteps,  # ç›´æ¥ä¼ å…¥æ•´æ•°æ—¶é—´æ­¥ï¼Œæ¨¡å‹å†…éƒ¨å¤„ç†åµŒå…¥
            content_condition_tokens=unrelated_opt
        )

        # 2. è®¡ç®—æŸå¤± (é¢„æµ‹çš„epsilon vs çœŸå®çš„epsilon)
        # è¿™å°±æ˜¯æ ‡å‡†çš„epsilon-prediction loss
        loss = criterion_mse(epsilon_pred, epsilon)

        loss.backward()
        optimizer.step()

        # ============================================
        #            éªŒè¯å¾ªç¯ (Validation Loop)
        # ============================================
        c_mdn.eval()
        val_loss_epoch = 0

        with torch.no_grad():  # éªŒè¯æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦
            for images, keys in tqdm(val_loader, desc=f"Stage 3 - Validation Epoch {epoch + 1}"):
        # ... (éªŒè¯é€»è¾‘ä¸è®­ç»ƒé€»è¾‘å‡ ä¹ä¸€æ ·ï¼Œä½†ä¸è¿›è¡Œä¼˜åŒ–) ...

        # a. æå–ç‰¹å¾
        # with torch.no_grad():
        #     ...
        #     unrelated_opt, related_opt_0 = ...

        # b. åŠ å™ª (ä½¿ç”¨ç›¸åŒçš„é€»è¾‘)
        # epsilon = torch.randn_like(related_opt_0)
        # timesteps = torch.randint(...)
        # noisy_related_opt_t = noise_scheduler.add_noise(...)

        # c. é¢„æµ‹epsilon
        # epsilon_pred = c_mdn(...)

        # d. è®¡ç®—æŸå¤±
        # loss = criterion_mse(epsilon_pred, epsilon)
        # val_loss_epoch += loss.item()

        avg_val_loss = val_loss_epoch / len(val_loader)

        print(f"Epoch {epoch + 1}/{args.num_epochs} | "
              f"Avg Train Loss: {avg_train_loss:.4f} | "
              f"Avg Val Loss: {avg_val_loss:.4f}")

        # ... (wandb æ—¥å¿—è®°å½• avg_train_loss å’Œ avg_val_loss) ...

        # ============================================
        #       æ¨¡å‹ä¿å­˜é€»è¾‘ (æ ¸å¿ƒä¿®æ”¹)
        # ============================================

        # 1. ä¿å­˜å½“å‰epochçš„æ¨¡å‹ (å¯é€‰ï¼Œç”¨äºæ–­ç‚¹ç»­è®­)
        # current_ckpt_path = os.path.join(args.output_dir, "c_mdn_latest.pt")
        # torch.save(c_mdn.state_dict(), current_ckpt_path)

        # 2. æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_ckpt_path = os.path.join(args.output_dir, "c_mdn_best.pt")
            torch.save(c_mdn.state_dict(), best_ckpt_path)
            print(f"ğŸ‰ New best model saved to: {best_ckpt_path} with Val Loss: {best_val_loss:.4f}")

    print("Stage 3 Conditional MDN training complete!")
    print(f"Best validation loss achieved: {best_val_loss:.4f}")

    # ... (æ—¥å¿—è®°å½•) ...

    print("Stage 3 Conditional MDN training complete!")
    # ä¿å­˜æ¨¡å‹
    output_path = os.path.join(args.output_dir, "c_mdn_stage3.pt")
    torch.save(c_mdn.state_dict(), output_path)
    print(f"Stage 3 C-MDN model saved to: {output_path}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Stage 3: Train a Conditional Modality Distribution Network")

    # --- è·¯å¾„å‚æ•° ---
    parser.add_argument("--stage1_ckpt_path", type=str, required=True, help="Path to Stage 1 checkpoint")
    parser.add_argument("--stage2_ckpt_path", type=str, required=True, help="Path to Stage 2 checkpoint")
    # ... å…¶ä»–è·¯å¾„å’Œé…ç½®å‚æ•° ...

    # --- æ¨¡å‹å‚æ•° ---
    # ... (éœ€è¦encoder_dimç­‰ä¸å‰ä¸¤é˜¶æ®µåŒ¹é…çš„å‚æ•°) ...

    # --- è®­ç»ƒå‚æ•° ---
    parser.add_argument("--num_epochs", type=int, default=200)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    # ... å…¶ä»–é€šç”¨å‚æ•° ...

    args = parser.parse_args()
    train_stage3_conditional_mdn(args)
